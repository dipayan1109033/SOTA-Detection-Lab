
# ----------------------------
# Experiment Configuration
# ----------------------------
exp:
  mode: train                       # Running mode: ['train', 'evaluate', 'predict']
  name: default                     # Experiment name for logging and model saving


# ----------------------------
# Dataset Configuration
# ----------------------------
data:
  folder: coco8                    # Name of the dataset folder (relative to dataset_root_dir)
  single_class: true               # If true, treats the dataset as a single-class detection task

  # Dataset Splitting
  split_ratios: [0.8, 0.2, 0.0]    # Ratios for train, validation, and test splits (should sum to 1.0)
  split_code: null                 # Custom split identifier (e.g., to reference a specific split configuration)
  test_split: val                  # Partition to be used for evaluation: ['val', 'test']

  # Cross-Validation Settings
  num_folds: 3                     # Number of folds for cross-validation
  use_replacement: false           # Whether to use replacement in cross-validation sampling

  # Data Augmentation Settings
  use_augmentation: false          # If true, applies data augmentation techniques during training


# ----------------------------
# Training Configuration
# ----------------------------
train:
  epoch: 3                          # Number of training epochs
  batch_size: 8                     # Number of samples per batch
  optimizer: SGD                    # Optimization algorithm (options: ['SGD', 'Adam'])
  lr: 0.01                          # Initial learning rate
  momentum: 0.937                   # Momentum factor (applicable for SGD)
  save_interval: -1                 # Interval for saving model checkpoints (A value of -1 disables this feature.)
  deterministic: true               # Helps improve reproducibility
  workers: 2                        # Number of data loading workers

  # Learning Rate Scheduler Configuration
  scheduler:
    type: OneCycleLR                # Learning rate scheduler type (options: ['StepLR', 'ExponentialLR', 'ReduceLROnPlateau', 'OneCycleLR'])
    max_lr: 0.01                    # Maximum learning rate for OneCycleLR
    div_factor: 25                  # Initial learning rate is max_lr / div_factor
    final_div_factor: 1e4           # Final learning rate is max_lr / final_div_factor
